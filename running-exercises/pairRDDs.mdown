
Working with Key-Value Pairs
=============================

While most Spark operations work on RDDs containing any type of objects, a few special operations are only
available on RDDs of key-value pairs. The most common ones are distributed "Shuffle" operations,
such as grouping or aggregating the elements by a key.

In Scala, these operations are automatically available on RDDs containing Tuple2 objects
(the built-in tuples in the language, created by simply writing (a,b)).

The key-value pair operations are available in the PairRDDFunctions class, which automatically
wraps around an RDD of tuples.

[PairRDDFunctions API](http://spark.apache.org/docs/1.5.0/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions)


## Creating a Pair RDD

```
scala> val ipAddressBytesRDD = successfulRequestsRDD.filter(record => record._7 != "-").map(record => (record._1,record._7.toInt))
ipAddressBytesRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[39] at map at <console>:30

scala> ipAddressBytesRDD.toDebugString
res73: String =
(2) MapPartitionsRDD[39] at map at <console>:30 []
 |  MapPartitionsRDD[38] at filter at <console>:30 []
 |  MapPartitionsRDD[13] at filter at <console>:28 []
 |  MapPartitionsRDD[12] at map at <console>:26 []
 |  MapPartitionsRDD[11] at textFile at <console>:21 []
 |  /user/gmedasani/data/access_log/access_log_1 HadoopRDD[10] at textFile at <console>:21 []
```

To check if this is a pairRDD let's run a operation reduceByKey() to see if it runs successfully.

```
scala> ipAddressBytesRDD.reduceByKey((value1,value2) => value1+value2).collectAsMap().toSeq.sortWith(_._2 > _._2)
res71: Seq[(String, Int)] = ArrayBuffer((64.242.88.10,4306193), (null,3379950), (10.0.0.153,1199844), (128.227.88.79,81785), (212.92.37.62,72981), (207.195.59.160,68210), (195.246.13.119,48692), (194.151.73.43,43518), (203.147.138.233,28136), (145.253.208.9,26098), (67.131.107.5,16465), (142.27.64.35,13164), (208.247.148.12,12268), (61.165.64.6,12224), (213.181.81.4,7649), (12.22.207.235,7368), (61.9.4.61,7368), (216.139.185.45,6051), (195.11.231.210,6032), (219.95.17.51,3169), (66.213.206.2,3169), (212.21.228.26,2869), (4.37.97.186,2446), (195.230.181.122,2300), (200.222.33.33,2300), (64.246.94.141,0), (64.246.94.152,0))

```

As we can see above, we are able to successfully create a key-value pairRDD and run a pairRDD function on it.


#### Here are some extra functions that are available on the RDDs of (key,value) pairs through an implicit conversion.


### Transformations on Pair RDDs

* aggregateByKey()
* combineByKey()
* countApproxDistinctByKey()
* countByKeyApprox()
* flatMapValues()
* foldByKey()
* groupByKey()
* groupWith()
* keys()
* mapValues()
* partitionBy()
* reduceByKey()
* reduceByKeyLocally()
* sampleByKey()
* values()
* saveAsNewAPIHadoopFile()


#### groupByKey([numTasks])

Meaning: When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. 

Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance. 

Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numTasks argument to set a different number of tasks.

> def groupByKey(): RDD[(K, Iterable[V])]

>> Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with the existing partitioner/parallelism level. The ordering of elements within each group is not guaranteed, and may even differ each time the resulting RDD is evaluated.


Current number of partitions/parallelism is 2

```
scala> ipAddressBytesRDD.partitions.length
res76: Int = 2
```
In this example, we can group by the Ipaddress and filter for each grouped ipAddress

```
scala> val groupedIPAddressRDD = ipAddressBytesRDD.groupByKey()
groupedIPAddressRDD: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[49] at groupByKey at <console>:32

```

We can now apply the following function to select only the values for each ip address where bytes are greater than 4000

```
scala> def selectBytesfourK(bytesIterable: Iterable[Int]): List[Int] = {
     |   val bytesIterator = bytesIterable.iterator
     |   var newBytesList = List[Int]()
     |   var currentValue = 0
     |   while(bytesIterator.hasNext){
     |     currentValue = bytesIterator.next
     |     if (currentValue > 4000){
     |       newBytesList = currentValue :: newBytesList}
     |   }
     |   newBytesList
     | }
selectBytesfourK: (bytesIterable: Iterable[Int])List[Int]

scala>

scala> val groupedIPAddressWithLargerBytes = groupedIPAddressRDD.map(record => (record._1,selectBytesfourK(record._2)))
groupedIPAddressWithLargerBytes: org.apache.spark.rdd.RDD[(String, List[Int])] = MapPartitionsRDD[56] at map at <console>:38

```

Another interesting there we can also see here is that there is a shuffle that happens at groupByKey operation. 

```
scala> groupedIPAddressWithLargerBytes.toDebugString
res80: String =
(2) MapPartitionsRDD[56] at map at <console>:38 []
 |  ShuffledRDD[55] at groupByKey at <console>:32 []
 +-(2) MapPartitionsRDD[39] at map at <console>:30 []
    |  MapPartitionsRDD[38] at filter at <console>:30 []
    |  MapPartitionsRDD[13] at filter at <console>:28 []
    |  MapPartitionsRDD[12] at map at <console>:26 []
    |  MapPartitionsRDD[11] at textFile at <console>:21 []
    |  /user/gmedasani/data/access_log/access_log_1 HadoopRDD[10] at textFile at <console>:21 []
```

Now when we run an action on the groupedIPAddressWithLargerBytes RDD, we see that it only triggers two tasks for groupByKey()

```
scala> groupedIPAddressWithLargerBytes.take(10)
res81: Array[(String, List[Int])] = Array((null,List(4062, 4062, 5672, 4022, 5234, 10392, 4583, 4750, 4232, 10419, 7649, 6051, 7649, 4114, 18767, 10392, 4564, 4731, 4213, 4154, 4004, 5234, 4377, 10392, 5383, 4619, 10419, 6697, 10419, 5691, 4034, 7368, 7939, 10936, 21125, 4615, 5253, 7435, 5543, 10392, 5691, 4034, 10419, 10419, 58169, 4646, 4154, 4004, 4213, 4515, 10392, 5672, 4022, 10392, 4866, 18753, 13973, 4901, 6697, 10419, 7368, 4156, 4534, 10392, 8667, 7368, 10936, 7939, 21125, 7368, 4449, 8793, 7461, 8331, 8380, 9622, 8605, 9263, 10936, 7939, 21125, 7939, 10936, 21125, 58169, 10313, 7461, 4646, 15147, 4004, 7411, 4449, 7529, 4377, 9109, 7762, 18859, 4472, 34910, 7410, 20420, 43816, 8347, 11733, 5567, 59549, 7875, 69197, 5420, 11355, 5794, 40430, 8986, 7912, 4300, 9812, 4430, 8193,...
scala>
```
![groupByKey with default partition parallelism](https://github.com/gdtm86/intel-advanced-analytics/blob/master/images/pairRDDs/groupByKey-1.jpg)

> def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]

>> Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with into numPartitions partitions. The ordering of elements within each group is not guaranteed, and may even differ each time the resulting RDD is evaluated.

>> Note: This operation may be very expensive. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using PairRDDFunctions.aggregateByKey or PairRDDFunctions.reduceByKey will provide much better performance.

>> Note: As currently implemented, groupByKey must be able to hold all the key-value pairs for any key in memory. If a key has too many values, it can result in an OutOfMemoryError.

Let's create a different RDD with groupByKey using 10 partitions.

```
scala> val groupedIPAddressRDD = ipAddressBytesRDD.groupByKey(10)
groupedIPAddressRDD: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[57] at groupByKey at <console>:32
```

```
scala> val groupedIPAddressWithLargerBytesRDD = groupedIPAddressRDD.map(record => (record._1,selectBytesfourK(record._2)))
groupedIPAddressWithLargerBytesRDD: org.apache.spark.rdd.RDD[(String, List[Int])] = MapPartitionsRDD[58] at map at <console>:38

scala> groupedIPAddressWithLargerBytesRDD.toDebugString
res82: String =
(10) MapPartitionsRDD[58] at map at <console>:38 []
 |   ShuffledRDD[57] at groupByKey at <console>:32 []
 +-(2) MapPartitionsRDD[39] at map at <console>:30 []
    |  MapPartitionsRDD[38] at filter at <console>:30 []
    |  MapPartitionsRDD[13] at filter at <console>:28 []
    |  MapPartitionsRDD[12] at map at <console>:26 []
    |  MapPartitionsRDD[11] at textFile at <console>:21 []
    |  /user/gmedasani/data/access_log/access_log_1 HadoopRDD[10] at textFile at <console>:21 []
```

Now when we run collect() action, we see 10 tasks.

```
scala> val groupedIPAddressWithLargerBytesLocal = groupedIPAddressWithLargerBytesRDD.collect()
val groupedIPAddressWithLargerBytesLocal = groupedIPAddressWithLargerBytesRDD.collect()
groupedIPAddressWithLargerBytesLocal: Array[(String, List[Int])] = Array((null,List(7368, 4156, 4534, 10392, 8667, 7368, 10936, 7939, 21125, 7368, 4449, 8793, 7461, 8331, 8380, 9622, 8605, 9263, 10936, 7939,...
```
![groupByKey with a parallelism of 10](https://github.com/gdtm86/intel-advanced-analytics/blob/master/images/pairRDDs/groupByKey-2.jpg)


#### reduceByKey(func, [numTasks])

When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.

> def reduceByKey(func: (V, V) â‡’ V, numPartitions: Int): RDD[(K, V)]
>> Merge the values for each key using an associative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a "combiner" in MapReduce. Output will be hash-partitioned with numPartitions partitions.

In the following example, we will sum all the bytes for a given ip address using reduceByKey operation. Since sum is an associative function, results are accurate though they are aggregated locally first and then globally.

```
scala> ipAddressBytesRDD
ipAddressBytesRDD
res87: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[39] at map at <console>:30

scala> ipAddressBytesRDD.reduceByKey((value1,value2) => (value1+value2)).take(10)
ipAddressBytesRDD.reduceByKey((value1,value2) => (value1+value2)).take(10)
res89: Array[(String, Int)] = Array((null,3379950), (195.246.13.119,48692), (195.11.231.210,6032), (219.95.17.51,3169), (212.92.37.62,72981), (142.27.64.35,13164), (212.21.228.26,2869), (12.22.207.235,7368), (194.151.73.43,43518), (200.222.33.33,2300))

```

#### agrregateByKey(zeroValue)(seqOp, combOp, [numTasks])

Meaning:  When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral "zero" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.

> defaggregateByKey[U](zeroValue: U)(seqOp: (U, V) â‡’ U, combOp: (U, U) â‡’ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
>> Aggregate the values of each key, using given combine functions and a neutral "zero value". This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U's, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.

One of the caveats of reduceByKey is the type of the values in the result RDD should be same as the source RDD. In aggregateRDD we can change those types by passing a SeqOP function that changes the types of values from V to U. 

Here we can see that the ipAddressBytesRDD is a pair RDD with keys of type String and values of type Int.

```
scala> ipAddressBytesRDD
ipAddressBytesRDD
res87: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[39] at map at <console>:30
```

By passing a zero value of 0.0 which is of type Double, we can see how we changed the type of the value in the output from Int to Double.

```
scala> ipAddressBytesRDD.aggregateByKey(0.0)((acc,value) => (acc+value),(acc1, acc2) => (acc1 + acc2)).take(2)
ipAddressBytesRDD.aggregateByKey(0.0)((acc,value) => (acc+value),(acc1, acc2) => (acc1 + acc2)).take(2)
res100: Array[(String, Double)] = Array((null,3379950.0), (195.246.13.119,48692.0))
```

#### sortByKey([ascending],[numTasks])

When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.

```
scala> val rdd = sc.parallelize(Seq(
     | ("math",    55),
     | ("math",    56),
     | ("english", 57),
     | ("english", 58),
     | ("science", 59),
     | ("science", 54)))
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[69] at parallelize at <console>:22

//Default Sorting : Ascending order
scala> val sorted1 = rdd.sortByKey()
val sorted1 = rdd.sortByKey()
sorted1: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[72] at sortByKey at <console>:24

//Result
scala> sorted1.collect()
sorted1.collect()
res101: Array[(String, Int)] = Array((english,57), (english,58), (math,55), (math,56), (science,54), (science,59))

//Default Sorting : Descending order (done using the 'ascending' flag argument)
scala> val sorted2 = rdd.sortByKey(false)
val sorted2 = rdd.sortByKey(false)
sorted2: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[75] at sortByKey at <console>:24

//Result
scala> sorted2.collect()
sorted2.collect()
res102: Array[(String, Int)] = Array((science,54), (science,59), (math,55), (math,56), (english,57), (english,58))
```

Note: We can even perform custom sorting by defining a new implicit ordering.
Also sortByKey() results in range-partitioned RDDs

sortByKey() is part of OrderedRDDFunctions that works on Key/Value pairs. You can learn more about OrderedRDDFunctions at 
[OrderedRDDFunctions API Documentation](http://spark.apache.org/docs/1.5.0/api/scala/index.html#org.apache.spark.rdd.OrderedRDDFunctions)

Custom sorting for the curious minds :)

Example 1:
```
scala> //Custom Sorting : Descending order (using implicit 'Ordering')
scala> {
     |    //Let us define an implicit sorting for the method sortByKey()
     |    //We have used '{' above to limit the scope of the implicit ordering
     |    implicit val sortIntegersByString = new Ordering[String] {
     |       override def compare(a: String, b: String) = {
     |          val result = a.compare(b)
     |          //We use -ve to sort the key in descending order
     |          -result
     |       }
     |    }
     |    val sorted2 = rdd.sortByKey()
     |
     |    //Result
     |    sorted2.collect()
     | }
```

Example 2:
```
import org.apache.spark.SparkContext._

val rdd: RDD[(String, Int)] = ...
implicit val caseInsensitiveOrdering = new Ordering[String] {
  override def compare(a: String, b: String) = a.toLowerCase.compare(b.toLowerCase)
}

// Sort by key, using the above case insensitive ordering.
rdd.sortByKey()
```

#### foldByKey() 

foldByKey() is very similar to fold() except that it operates on a Pair RDD

> def foldByKey(zeroValue: V)(func: (V, V) â‡’ V): RDD[(K, V)]
>> Merge the values for each key using an associative function and a neutral "zero value" which may be added to the result an arbitrary number of times, and must not change the result (e.g., Nil for list concatenation, 0 for addition, or 1 for multiplication.).

```
scala> sorted2.foldByKey(0)((value1,value2) => (value1 + value2))
sorted2.foldByKey(0)((value1,value2) => (value1 + value2))
res104: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[76] at foldByKey at <console>:27
```

"Zero value" should be of the same type as the type of the values in Pair RDDs.

```
scala> sorted2.foldByKey(0.0)((value1,value2) => (value1 + value2)).collect()
sorted2.foldByKey(0.0)((value1,value2) => (value1 + value2)).collect()
<console>:27: error: type mismatch;
 found   : Double(0.0)
 required: Int
              sorted2.foldByKey(0.0)((value1,value2) => (value1 + value2)).collect()
                                ^
```

#### mapValues()

> def mapValues[U](f: (V) â‡’ U): RDD[(K, U)]
>> Pass each value in the key-value pair RDD through a map function without changing the keys; this also retains the original RDD's partitioning.

When we use map() with a Pair RDD, we get access to both Key & value. There are times we might only be interested in accessing the value(& not key). In those case, we can use mapValues() instead of map().

In this example we use mapValues() along with reduceByKey() to calculate average for each subject

```
scala> val inputrdd = sc.parallelize(Seq(("maths", 50), ("maths", 60), ("english", 65)))
val inputrdd = sc.parallelize(Seq(("maths", 50), ("maths", 60), ("english", 65)))
inputrdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[79] at parallelize at <console>:22

scala> val mapped = inputrdd.mapValues(mark => (mark, 1));
val mapped = inputrdd.mapValues(mark => (mark, 1));
mapped: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[80] at mapValues at <console>:24

scala> mapped.take(2)
mapped.take(2)
res109: Array[(String, (Int, Int))] = Array((maths,(50,1)), (maths,(60,1)))

scala> val reduced = mapped.reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2))
val reduced = mapped.reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2))
reduced: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[81] at reduceByKey at <console>:28

scala>val average = reduced.map { x =>
     | val temp = x._2
     | val total = temp._1
     | val count = temp._2
     | (x._1, total / count)
     |
}
average: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[83] at map at <console>:30

scala> average.collect()
average.collect()
res110: Array[(String, Int)] = Array((english,65), (maths,55))
```

Note: Operations like map() always cause the new RDD to no retain the parent partitioning information


#### flatMapValues()

Applies a function that returns an iterator to each value of a pair RDD, and for each element returned, produce a key/value entry with old key. Often used for tokenization.

> def flatMapValues[U](f: (V) â‡’ TraversableOnce[U]): RDD[(K, U)]
>> Pass each value in the key-value pair RDD through a flatMap function without changing the keys; this also retains the original RDD's partitioning.

```
scala> val inputRDD = sc.parallelize(List((1, 2), (3, 4), (3, 6)))
val inputRDD = sc.parallelize(List((1, 2), (3, 4), (3, 6)))
inputRDD: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[86] at parallelize at <console>:22

scala> inputRDD.flatMapValues( x => (x to 5)).collect()
inputRDD.flatMapValues( x => (x to 5)).collect()
res116: Array[(Int, Int)] = Array((1,2), (1,3), (1,4), (1,5), (3,4), (3,5))

```

#### keys()
Returns an RDD of just the keys

>def keys: RDD[K]
>> Return an RDD with the keys of each tuple.

```
scala> inputRDD.keys.take(4)
inputRDD.keys.take(4)
res121: Array[Int] = Array(1, 3, 3)
```

#### values()
Returns an RDD of just the values
> def values: RDD[V]
>> Return an RDD with the values of each tuple.

```
scala> inputRDD.values.take(4)
inputRDD.values.take(4)
res122: Array[Int] = Array(2, 4, 6)
```

### Transformations on two Pair RDDs

* subtractByKey()
* join()
* fullOuterJoin()
* rightOuterJoin()
* leftOuterJoin()
* cogroup()

#### subtractByKey()

Remove elements with a key present in the other RDD.

> def subtractByKey[W](other: RDD[(K, W)])(implicit arg0: ClassTag[W]): RDD[(K, V)]
>> Return an RDD with the pairs from this whose keys are not in other.

>> Uses this partitioner/partition size, because even if other is huge, the resulting RDD will be <= us.

```
scala> val rdd1 = sc.parallelize(List((1, 2), (3, 4), (3, 6)))
rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[1] at parallelize at <console>:21

scala> val rdd2 = sc.parallelize(List((3,9)))
rdd2: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[3] at parallelize at <console>:21

scala> rdd1.subtractByKey(rdd2)
res1: org.apache.spark.rdd.RDD[(Int, Int)] = SubtractedRDD[4] at subtractByKey at <console>:26

scala> rdd1.subtractByKey(rdd2).collect()
res2: Array[(Int, Int)] = Array((1,2))

```

#### join()

join(otherDataset, [numTasks])	When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.

Perform an inner join between two RDDs.

> def join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))]
>> Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster.

```
scala> rdd1.join(rdd2)
res3: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = MapPartitionsRDD[8] at join at <console>:26

scala> rdd1.join(rdd2).collect()
res4: Array[(Int, (Int, Int))] = Array((3,(4,9)), (3,(6,9)))
```

#### fullOuterJoin(()

> def fullOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (Option[V], Option[W]))]
>> Perform a full outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in other, or the pair (k, (Some(v), None)) if no elements in other have key k. Similarly, for each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for v in this, or the pair (k, (None, Some(w))) if no elements in this have key k. Hash-partitions the resulting RDD into the given number of partitions.


```

scala> rdd1.fullOuterJoin(rdd2)
res5: org.apache.spark.rdd.RDD[(Int, (Option[Int], Option[Int]))] = MapPartitionsRDD[14] at fullOuterJoin at <console>:26

scala> rdd1.fullOuterJoin(rdd2).collect()
res6: Array[(Int, (Option[Int], Option[Int]))] = Array((1,(Some(2),None)), (3,(Some(6),Some(9))), (3,(Some(4),Some(9))))

scala>

```

#### rightOuterJoin()

Perform a join between two RDDs where the key must be present in the first RDD.

> def rightOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], W))]
>> Perform a right outer join of this and other. For each element (k, w) in other, the resulting RDD will either contain all pairs (k, (Some(v), w)) for v in this, or the pair (k, (None, w)) if no elements in this have key k. Hash-partitions the resulting RDD using the existing partitioner/parallelism level.

```
scala> rdd1.rightOuterJoin(rdd2).collect()
res8: Array[(Int, (Option[Int], Int))] = Array((3,(Some(4),9)), (3,(Some(6),9)))

```

#### leftOuterJoin()

Perform a join between two RDDs where the key must be present in the other RDD.

> def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]
>> Perform a left outer join of this and other. For each element (k, v) in this, the resulting RDD will either contain all pairs (k, (v, Some(w))) for w in other, or the pair (k, (v, None)) if no elements in other have key k. Hash-partitions the output using the existing partitioner/parallelism level.

```
scala> rdd1.leftOuterJoin(rdd2).collect()
res9: Array[(Int, (Int, Option[Int]))] = Array((1,(2,None)), (3,(4,Some(9))), (3,(6,Some(9))))
```

#### cogroup()

Group data from both RDDs sharing the same key.

>def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]
>> For each key k in this or other, return a resulting RDD that contains a tuple with the list of values for that key in this as well as other.

```
scala> rdd1.cogroup(rdd2).collect()
res11: Array[(Int, (Iterable[Int], Iterable[Int]))] = Array((1,(CompactBuffer(2),CompactBuffer())), (3,(CompactBuffer(4, 6),CompactBuffer(9))))

```

### Actions Available on Pair RDDs

* lookup()
* collectAsMap()
* countByKey()

#### collectAsMap()

> def collectAsMap(): Map[K, V]
>> Return the key-value pairs in this RDD to the master as a Map.

>> Warning: this doesn't return a multimap (so if you have multiple values to the same key, only one value per key is preserved in the map returned)

```
scala> val ipAddressMap = ipAddressBytesRDD.reduceByKey((value1,value2) => value1+value2).collectAsMap()
ipAddressMap: scala.collection.Map[String,Int] = Map(12.22.207.235 -> 7368, 64.242.88.10 -> 4306193, 203.147.138.233 -> 28136, 67.131.107.5 -> 16465, 195.246.13.119 -> 48692, 213.181.81.4 -> 7649, 195.230.181.122 -> 2300, 64.246.94.141 -> 0, 10.0.0.153 -> 1199844, 212.92.37.62 -> 72981, 128.227.88.79 -> 81785, 208.247.148.12 -> 12268, 4.37.97.186 -> 2446, 212.21.228.26 -> 2869, 216.139.185.45 -> 6051, 207.195.59.160 -> 68210, 219.95.17.51 -> 3169, 195.11.231.210 -> 6032, 61.9.4.61 -> 7368, 194.151.73.43 -> 43518, 61.165.64.6 -> 12224, 64.246.94.152 -> 0, 142.27.64.35 -> 13164, 200.222.33.33 -> 2300, 66.213.206.2 -> 3169, 145.253.208.9 -> 26098, null -> 3379950)
```

#### countByKey()

Only available on RDDs of type (K,V). Returns a hashmap of (K,Int) pairs with the count of each key.

> def countByKey(): Map[K, Long]
>> Count the number of elements for each key, collecting the results to a local Map.

Note that this method should only be used if the resulting map is expected to be small, as the whole thing is loaded into the driver's memory. To handle very large results, consider using rdd.mapValues(_ => 1L).reduceByKey(_ + _), which returns an RDD[T, Long] instead of a map.


In the below example, we can see that countByKey() returns the counts of values by each key (ipaddress)

```
scala> val ipAddressCount = ipAddressBytesRDD.countByKey()
ipAddressCount: scala.collection.Map[String,Long] = Map(null -> 646, 203.147.138.233 -> 13, 212.92.37.62 -> 14, 142.27.64.35 -> 2, 61.165.64.6 -> 4, 64.242.88.10 -> 340, 66.213.206.2 -> 1, 4.37.97.186 -> 1, 216.139.185.45 -> 1, 212.21.228.26 -> 1, 145.253.208.9 -> 6, 219.95.17.51 -> 1, 200.222.33.33 -> 1, 67.131.107.5 -> 3, 195.246.13.119 -> 11, 64.246.94.152 -> 1, 195.11.231.210 -> 1, 64.246.94.141 -> 1, 208.247.148.12 -> 4, 207.195.59.160 -> 14, 10.0.0.153 -> 187, 194.151.73.43 -> 4, 61.9.4.61 -> 1, 213.181.81.4 -> 1, 12.22.207.235 -> 1, 128.227.88.79 -> 12, 195.230.181.122 -> 1)

```

#### lookup() 

> def lookup(key: K): Seq[V]
>> Return the list of values in the RDD for key key. This operation is done efficiently if the RDD has a known partitioner by only searching the partition that the key maps to.


We can use the lookup function to lookup the values for a particular key. 

For example, from the previous countByKey result '203.147.138.233 -> 13' we see that the ip address 203.147.138.233 has 13 entries.

Let's loookup the bytes values for this ipaddress

```
scala> ipAddressBytesRDD.lookup("203.147.138.233")
res75: Seq[Int] = WrappedArray(2955, 1078, 3041, 1695, 2577, 3203, 1970, 2181, 1550, 2314, 1850, 2213, 1509)

```


#### Advanced for later:

##### CombineByKey()

