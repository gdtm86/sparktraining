
Text Analytics and Entity Resolution
====================================

### Introduction

Entity Resolution is a common, yet difficult problem in data cleaning and data integration.This lab will demonstrate how we can use Apache Spark to apply powerful and scalable text analysis techniques and perform entity resolution across two datasets of commercial products

### What is Entity Resolution?

Entity Resolution, or "[Record linkage](https://en.wikipedia.org/wiki/Record_linkage)" is the term used by statisticians, epidemiologists, and historians, among others, to describe the process of joining records from one data source with another that describe the same entity. 

Our terms with the same meaning include, "entity disambiguation/linking", duplicate detection", "deduplication", "record matching", "(reference) reconciliation", "object identification", "data/information integration", and "conflation".

Entity Resolution (ER) refers to the task of finding records in a dataset that refer to the same entity across different data sources (e.g., data files, books, websites, databases). 

ER is necessary when joining datasets based on entities that may or may not share a common identifier (e.g., database key, URI, National identification number), as may be the case due to differences in record shape, storage location, and/or curator style or preference. A dataset that has undergone ER may be referred to as being cross-linked

#### DataSet

Dataset for this assignment is from the [metric-learning](https://code.google.com/p/metric-learning/) project.

* Google.csv -  the Google Products dataset
* Amazon.csv - the Amazon dataset
* Google_small.csv - 200 records sampled from the Google data
* Amazon_small.csv - 200 records sampled from the Amazon data
* Amazon_Google_perfectMapping.csv - the "gold standard" mapping
* stopwords.txt - a list of common English words


#### DataSet format

The file format of an Amazon line is:  
> "id","title","description","manufacturer","price”

The file format of a Google line is
> `"id","name","description","manufacturer","price”

### Part 1: Load the Parse the datasets correctly

Use the utility class provided ProductDataParserUtil to parse the data in the right format. 

Function parseDatafileLine() returns an RDD of type ((productID,productDetails),successCode)

* successCode (0) – means it is the header row/record
* successCode (1) – means it is a valid record
* successCode (-1) – means it is an invalid record

You can use the successCodes to create a valid RDD for each data source.

***For example: validAmazonSmallRDD, validGoogleSmallRDD, validGoogleRDD, validAmazonRDD.***

Each of these RDDs should be of the type (productID,productDetails), we can drop the success code after the filtering. 

##### Verify

```
Google_small.csv - Read 201 lines, successfully parsed 200 lines, failed to parse 0 lines 
Google.csv - Read 3227 lines, successfully parsed 3226 lines, failed to parse 0 lines 
Amazon_small.csv - Read 201 lines, successfully parsed 200 lines, failed to parse 0 lines 
Amazon.csv - Read 1364 lines, successfully parsed 1363 lines, failed to parse 0 lines
```

Once the data is parsed correctly please take some time to examine the data and understand how the data looks like.

### Part 2: Create tokens from documents

#### Entity Resolution as Text Similarity - Bag of Words

A simple approach to entity resolution is to treat all records as strings and compute their similarity with a string distance function. 

In this part, we will build some components for performing bag-of-words text-analysis, and then use them to compute record similarity.

Bag-of-Words is a conceptually simple yet powerful approach to text analysis.The idea is to treat strings, a.k.a. documents, as unordered collections of words, or tokens, i.e., as bags of words.

**Note on terminology**: a "token" is the result of parsing the document down to the elements we consider "atomic" for the task at hand.  

Bag of words techniques all apply to any sort of token, so when we say "bag-of-words" we really mean "bag-of-tokens," strictly speaking.Tokens become the atomic unit of text comparison. If we want to compare two documents, we count how many tokens they share in common.

If we want to search for documents with keyword queries (this is what Google does), then we turn the keywords into tokens and find documents that contain them. The power of this approach is that it makes string comparisons insensitive to small differences that probably do not affect meaning much, for example, punctuation and word order.

[Bag-of-words Model](https://en.wikipedia.org/wiki/Bag-of-words_model)


#### Creating Bag-of-Words

Use the utility function **tokenize()** provided in **ProductDataParserUtil** class to tokenize the productDetails part of the validRDDs for each datasets.

To keep the project run times faster, we will just tokenize the **validGoogleSmallRDD** and **validAmazonSmallRDD** for now.

Output of this process should be two RDDs **amazonSmallTokensRDD** and **googleSmallTokensRDD**

##### Verify

Total Number of tokens in the *amazonSmallTokensRDD*  and *googleSmallTokensRDD* 

There are **22520** tokens in the combined datasets 


### Part 3: Calculate TF-IDF weights for words

Bag-of-words comparisons are not very good when all tokens are treated the same: some tokens are more important than others. 

Weights give us a way to specify which tokens to favor. With weights, when we compare documents, instead of counting common tokens, we sum up the weights of common tokens. 

A good heuristic for assigning weights is called **"Term-Frequency/Inverse-Document-Frequency,"** or **TF-IDF(tfidf)** for short.

#### Term Frequency

TF rewards tokens that appear many times in the same document. 

It is computed as the frequency of a token in a document, that is, if document **d** contains **100** tokens and token **t** appears in **d** **5** times, then the **TF** weight of t in d is 5/100 = 1/20. 

The intuition for TF is that if a word occurs often in a document, then it is more important to the meaning of the document.

#### Inverse Document Frequency

IDF rewards tokens that are rare overall in a dataset. The intuition is that it is more significant if two documents share a rare word than a common one.

IDF weight for a token, **t**, in a set of documents, **U**, is computed as follows:

* Let N be the total number of documents in U
* Find n(t), the number of documents in U that contain t 
* Then IDF(t) = N/n(t).

Note that **n(t)/N** is the frequency of **t** in **U**, and **N/n(t)** is the inverse frequency.


#### TF-IDF Note on Terminology 

Sometimes token weights depend on the document the token belongs to, that is, the same token may have a different weight when it's found in different documents.  We call these weights **local** weights. TF is an example of a local	 weight, because it depends on the length of the source.  

On the other hand, some token weights only depend on the token, and are the same everywhere that token is found.  We call these weights **global**, and IDF is one such weight.

**[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)** : Finally, to bring it all together, the total TF-IDF weight for a token in a document is the product of its TF and IDF weights. 

#### Create a function called termFrequency()

This function will take a list of tokens 
Function should return the frequency for each token in the list. Formula to calculate the term frequency is in the Part 3: Term Frequency slide

This is also easy to test. You can just pass a list of tokens into the function and manually verify if it is printing the right frequency.


For example: a list of ['quick','brown','fox','jumps','lazy','dog’] should return -

```
(brown, 0.16666666666666666), (lazy, 0.16666666666666666), (jumps,0.16666666666666666), (fox,0.16666666666666666), (dog,0.16666666666666666), (quick, 0.16666666666666666)
```

A list of [one,one,two] should return

```
(one,0.6666666666666666),(two,0.3333333333333333)
```

#### Create a combined Small dataset

Create a *smallCorpusRDD* that is a combined dataset of *googleSmallTokensRDD* and *amazonSmallTokensRDD*

You can use union for this.

*smallCorpusRDD* should have a total of **400** records.


#### Create a function called inverseDocumentFrequency()

This function will take 
> Args:        corpus (RDD): input corpus   
> Returns:        RDD: a RDD of (token, IDF value)

Hint: This also uses the concepts we saw in the wordCount and log analysis exercises


#### Apply inverseDocumentFrequency() on smallCorpusRDD 

Create an **idfSmallRDD** by applying *inverseDocumentFrequency()* on *smallCorpusRDD*

To verify if your IDF is correct – run a count() on the smallCorpusRDD, you should see that a total of **4772** unique tokes in the idfSmallRDD 

Collect all the idfs from idfSmallRDD into a variable called **idfSmallWeights** using the collectAsMap() 

#### Verify:

Print out 11 tokens with smallest IDF from the **idfSmallRDD**

Correct answer should have the following tokes and idf weights

```
('software', 4.25531914893617), ('new', 6.896551724137931), ('features', 6.896551724137931), ('use', 7.017543859649122), ('complete', 7.2727272727272725), ('easy', 7.6923076923076925), ('create', 8.333333333333334), ('system', 8.333333333333334), ('cd', 8.333333333333334), ('1', 8.51063829787234), ('windows', 8.51063829787234)
```

#### Create a function called tfIDF()

This function will take

> Args:
>> tokens (list of str): input list of tokens from tokenize

>> idfs (Map[String,Float]): record to IDF value

> Returns:
>> tfidfs: A Map[String,Float] of tokens to TF-IDF values


Create a RDD called **tfIDFSmallRDD** that contains the TF-IDFs for all the tokens in the *smallCorpusRDD*

#### Verify

We can look at the following record 'b000hkgj8k' from amazonSmallTokensRDD

```
tf_idfSmallRDD.filter(record => record._1 == "b000hkgj8k").collect().foreach(println)
```



